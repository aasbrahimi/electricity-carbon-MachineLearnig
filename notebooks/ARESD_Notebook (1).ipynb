{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARESD - CO2 emissions from the electricity mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This classâ€™s objective is to predict the carbon quality of the electricity generation mix (Carbonised, Normal, Decarbonised) given weather and calendar variables by using different models and comparing their accuracy rates to decide which one is the most reliable one.\n",
    "#### Throughout this report, we will explore the principles of the Bayes Classifier, kNN, and Decision Tree, aiming to gain insights into their roles in modern data analysis and classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library import and data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn \n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmark = pd.read_csv(\"benchmark.csv\")\n",
    "path_to_data = \"/kaggle/input/are-sd-2024-emissions-co2-du-mix-electrique/\"\n",
    "train = pd.read_csv(path_to_data + \"train.csv\")\n",
    "test = pd.read_csv(path_to_data + \"prev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions needed to study the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Class Accuracy\n",
    "def ARE(prev,true,v = 'MixProdElec'):   #The variable v is the one to predict, here our study concerns the variable \"MixProdElec\"\n",
    "    \"\"\"\n",
    "    Calculates the precision of our model\n",
    "\n",
    "    Parameters :\n",
    "        prev : the predicted labels\n",
    "        true : the true labels\n",
    "\n",
    "    Returns :\n",
    "        float : the accuracy rate\n",
    "    \"\"\"\n",
    "    return (100/len(true))*(np.sum(true==prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "def confusion_matrix(actual, predicted):\n",
    "    \"\"\"\n",
    "    Generates a confusion matrix\n",
    "\n",
    "    Parameters:\n",
    "        actual: the true labels\n",
    "        predicted: the predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        np.array: the confusion matrix\n",
    "    \"\"\"\n",
    "    labels = np.unique(actual)       #We assume that actual and predicted have the same labels\n",
    "    num_labels = len(labels)\n",
    "    conf = np.zeros((num_labels, num_labels))\n",
    "    \n",
    "    for a, p in zip(actual, predicted):\n",
    "        conf[a, p] += 1\n",
    "    \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset\n",
    "def split_train_test(X,y,ratio_train=0.8):  #It can also be done randomly, here we decided to choose ourselves\n",
    "    X_train= X[:int(ratio_train*len(X))]\n",
    "    X_test= X[int(ratio_train*len(X)):]\n",
    "    y_train= y[:int(ratio_train*len(y))]\n",
    "    y_test = y[int(ratio_train*len(y)):]\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoder\n",
    "def label_encoder(feature_vec):\n",
    "    \"\"\"\n",
    "    Encoding categorical labels to integers (for example for the feature \"MixProdElec\" : [\"Normal\", \"Carbonne\", \"Decarbonne\"] -> [0,1,2])\n",
    "    \"\"\"\n",
    "    unique_labels = set(feature_vec)\n",
    "    index = [i for i in range(len(unique_labels))]\n",
    "    label_to_index = {label: index for (label,index) in zip(unique_labels,index)}\n",
    "    encoded_labels = [label_to_index[label] for label in feature_vec]\n",
    "    return np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label decoder\n",
    "def label_decoder(encoded_labels, original_labels):\n",
    "    \"\"\"\n",
    "    Decodes encoded integer labels back to their original categorical labels.\n",
    "    \"\"\"\n",
    "    # Create a reverse mapping from encoded labels to original labels\n",
    "    index_to_label = {i: label for i, label in enumerate(original_labels)}\n",
    "    # Decode the encoded labels back to their original categorical labels\n",
    "    decoded_labels = np.array([index_to_label[label] for label in encoded_labels])\n",
    "    return decoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crossed probability law\n",
    "def cond_prob_table(data, v1, v2 = \"MixProdElec\"):\n",
    "    \"\"\"\n",
    "    Calculates the crossed probability law of 2 variables\n",
    "\n",
    "    Parameters:\n",
    "        data : the dataset\n",
    "        v1 : the first variable\n",
    "        v2 : the second variable\n",
    "        \n",
    "    Returns :\n",
    "        np.array : the conditional probability table\n",
    "    \"\"\"\n",
    "    x = data[v1]\n",
    "    y = data[v2]\n",
    "    n_x = len(np.unique(x))\n",
    "    n_y = len(np.unique(y))\n",
    "    pxy =np.zeros((n_x,n_y))\n",
    "    \n",
    "    for i in range(n_x):\n",
    "        for j in range(n_y):\n",
    "            pxy[i,j]= np.mean((data[v1] == np.unique(data[v1])[i])*(data[v2] == np.unique(data[v2])[j]))\n",
    "    return pxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Independence test\n",
    "def independence_test(pxy):\n",
    "    \"\"\"\n",
    "    Checks whether two variables are independent\n",
    "    \"\"\"\n",
    "    px = np.array([np.sum(pxy,axis =1)])\n",
    "    py = np.array([np.sum(pxy,axis = 0)])\n",
    "    pxpy= px.T*py\n",
    "\n",
    "    for i in range(len(pxy)):\n",
    "        for j in range(len(pxy[i])):\n",
    "            if pxy[i,j] != pxpy[i,j]:\n",
    "                return False\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes Classifier\n",
    "def Bayes_Classif(train,test,v):\n",
    "    \"\"\"\n",
    "    Naive Bayes Implementation\n",
    "    \n",
    "    Parameters:\n",
    "        train : the training dataset (the function will be trained on this dataset)\n",
    "        test : the test dataset (the function will make predictions on this dataset)\n",
    "        v : the variable for which the classification is performed\n",
    "    \n",
    "    Returns :\n",
    "        np.array : the predicted classes from the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    pxy = cond_prob_table(train,v)\n",
    "    #classes = np.unique(train['MixProdElec'])\n",
    "    values = np.unique(train[v])\n",
    "    X = test[v]\n",
    "    pred = np.empty(len(X))  #creates a new array of shape len(X)\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        pred[i] = np.argmax(pxy[np.argmax(values == X.iloc[i]),:])\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modulo distance\n",
    "def modulo_distance(xTrain, xTest, modulo = 7):\n",
    "    diff = np.abs(xTrain[:, np.newaxis] - xTest)\n",
    "    squared_distance = (modulo % (diff +1e-7))**2\n",
    "    distances = squared_distance\n",
    "    distances[distances < 0] = 0\n",
    "    distances = np.sqrt(distances)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Euclidean distance\n",
    "def euclidean_distance(xTrain, xTest):\n",
    "    distances = np.sqrt(np.sum((xTrain[:, np.newaxis] - xTest) ** 2, axis=-1))\n",
    "    distances[distances < 0] = 0\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hamming distance\n",
    "def hamming_distance(xTrain, xTest):\n",
    "    \"\"\"\n",
    "    Hamming distance is used for comparing two strings of equal length. \n",
    "    It calculates the number of positions at which the corresponding symbols are different.\n",
    "    \"\"\"\n",
    "    distances = np.sum(xTrain != xTest, axis=-1)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, X_test, k):\n",
    "    \"\"\"\n",
    "    Find the k-nearest neighbors for each test sample in the batch.\n",
    "    \"\"\"\n",
    "    distances = euclidean_distance(X_test, X_train)\n",
    "    return np.argsort(distances, axis=1)[:, :k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN Classification\n",
    "def knn_classification(X_train, y_train, X_test, k, batch_size=100):\n",
    "    \"\"\"\n",
    "    Make predictions for the test samples using k-nearest neighbors.\n",
    "    \"\"\"\n",
    "    n_test_samples = X_test.shape[0]\n",
    "    y_pred = np.empty(n_test_samples, dtype=int)\n",
    "\n",
    "    for i in tqdm(range(0, n_test_samples, batch_size)):\n",
    "        batch = X_test[i:i+batch_size]\n",
    "        neighbors = knn(X_train, batch, k)\n",
    "        y_pred[i:i+batch_size] = np.array([np.argmax(np.bincount(y_train[indices])) for indices in neighbors])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the data\n",
    "train=data[:int((len(data)*1)*.36505)+1]\n",
    "test=data[int((len(data)*1)*.63495):]\n",
    "\n",
    "n=train.shape[0]\n",
    "X_train = train.sample(n, random_state = 2023).iloc[:,2:]\n",
    "y_train = train.sample(n, random_state = 2023).iloc[:,0]\n",
    "nt=test.shape[0]\n",
    "X_test = test.iloc[:nt,2:]\n",
    "y_test = test.iloc[:nt,0]\n",
    "\n",
    "labels = ['Jour', 'Mois', 'Jour','JourFerie','JourFerieType',\n",
    " 'VacancesZoneA','VacancesZoneB','VacancesZoneC' ]\n",
    "\n",
    "for i in labels:\n",
    "    X_train[i] = label_encoder(X_train[i].values)\n",
    "    X_test[i]  = label_encoder(X_test[i].values)\n",
    "    \n",
    "y_train = label_encoder(y_train.values)\n",
    "y_test = label_encoder(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using knn\n",
    "y_pred = knn_classification(X_train.values.astype(np.float32), y_train.astype(np.int8), X_test.values.astype(np.float32),5)\n",
    "decoded_y_pred = label_decoder(y_pred, ['Carbonne', 'Decarbonne', 'Normal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini impurity : measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(labels):\n",
    "    \"\"\"\n",
    "    Calculate the Gini impurity for a set of labels.\n",
    "\n",
    "    Parameters:\n",
    "        labels (numpy.ndarray): An array of labels.\n",
    "\n",
    "    Returns:\n",
    "        float: The Gini impurity value.\n",
    "    \"\"\"\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    return 1 - np.sum(probabilities ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, labels, feature_index, threshold):\n",
    "    \"\"\"\n",
    "    Split the data and labels based on a feature and threshold.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The input data.\n",
    "        labels (numpy.ndarray): The corresponding labels.\n",
    "        feature_index (int): The index of the feature to split on.\n",
    "        threshold (float): The threshold value for splitting.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the left and right split of data and labels.\n",
    "    \"\"\"\n",
    "    left_mask = data[:, feature_index] < threshold\n",
    "    right_mask = ~left_mask\n",
    "    return data[left_mask], labels[left_mask], data[right_mask], labels[right_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best split : iterates over all features and thresholds to find the split that minimizes the Gini impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(data, labels):\n",
    "    \"\"\"\n",
    "    Find the best split for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The input data.\n",
    "        labels (numpy.ndarray): The corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the best feature index and threshold value.\n",
    "    \"\"\"\n",
    "    best_feature, best_threshold, best_gini = None, None, float('inf')\n",
    "    for feature_index in range(data.shape[1]):\n",
    "        thresholds = np.unique(data[:, feature_index])\n",
    "        for threshold in thresholds:\n",
    "            _, left_labels, _, right_labels = split_data(data, labels, feature_index, threshold)\n",
    "            if len(left_labels) == 0 or len(right_labels) == 0:\n",
    "                continue\n",
    "            gini = (len(left_labels) * gini_impurity(left_labels) +\n",
    "                    len(right_labels) * gini_impurity(right_labels)) / len(labels)\n",
    "            if gini < best_gini:\n",
    "                best_feature, best_threshold, best_gini = feature_index, threshold, gini\n",
    "    return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree : it recursively builds the decision tree by finding the best split at each node until the maximum depth is reached or the minimum number of samples is not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, labels, max_depth, min_samples_split, depth=0):\n",
    "    \"\"\"\n",
    "    Build a decision tree recursively.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The input data.\n",
    "        labels (numpy.ndarray): The corresponding labels.\n",
    "        max_depth (int): The maximum depth of the tree.\n",
    "        min_samples_split (int): The minimum number of samples required to split a node.\n",
    "        depth (int, optional): The current depth of the tree. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: The decision tree represented as a dictionary.\n",
    "    \"\"\"\n",
    "    if depth == max_depth or len(labels) < min_samples_split or gini_impurity(labels) == 0:\n",
    "        return {'prediction': np.argmax(np.bincount(labels))}\n",
    "    feature, threshold = find_best_split(data, labels)\n",
    "    if feature is None:\n",
    "        return {'prediction': np.argmax(np.bincount(labels))}\n",
    "    left_data, left_labels, right_data, right_labels = split_data(data, labels, feature, threshold)\n",
    "    return {\n",
    "        'feature': feature,\n",
    "        'threshold': threshold,\n",
    "        'left': build_tree(left_data, left_labels, max_depth, min_samples_split, depth + 1),\n",
    "        'right': build_tree(right_data, right_labels, max_depth, min_samples_split, depth + 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction : it traverses the decision tree based on the feature values of the data point and returns the predicted class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, data_point):\n",
    "    \"\"\"\n",
    "    Make a prediction for a single data point using the decision tree.\n",
    "\n",
    "    Args:\n",
    "        tree (dict): The decision tree represented as a dictionary.\n",
    "        data_point (numpy.ndarray): The input data point.\n",
    "\n",
    "    Returns:\n",
    "        int: The predicted class label.\n",
    "    \"\"\"\n",
    "    if 'prediction' in tree:\n",
    "        return tree['prediction']\n",
    "    if data_point[tree['feature']] < tree['threshold']:\n",
    "        return predict(tree['left'], data_point)\n",
    "    else:\n",
    "        return predict(tree['right'], data_point)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
